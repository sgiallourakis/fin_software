{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ebfcd2a-54c1-4be7-b6a7-5aea7a640d46",
   "metadata": {},
   "source": [
    "This file is used testing new bit of code before. Please label all code accordingly. Describe what each cell of code is intended to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d45eb2-b0c0-4a32-bd76-dd78c24f6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Testing - Created 6-23-2024\n",
    "# Using Pearson Correlation Coefficient First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9abc799a-4c85-4735-af36-d7fafdac8ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries needed for analysis.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2825e-3f27-4ed2-87ac-6fb553509f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion\n",
    "# If data new data needs to be pulled from Alpha Vantage use function av_pull\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# json_to_df -- converts json pulled from alphvantage and converts into a dataframe. It also adds a title to columns that contains the date and names it \"date\".\n",
    "# df_name will be \"symbol_df\", example TASEKO Mines symbol is TGB, therefore df_name would be TGB_df\n",
    "\n",
    "\n",
    "     \n",
    "# the function contained in this cell pertain to statistical analysis code, and specifically correlation testing\n",
    "def csv_to_df(file_name, symbol):\n",
    "    df = pd.read_csv(file_name, parse_dates['Date'], index_col = 'Date')\n",
    "    return {symbol: df}\n",
    "\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e06689-cb11-425e-ae57-c96996accbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code creates the dictionary to store our Dataframes\n",
    "stock_dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1cdf4f-2f8c-4096-ae27-3861fcd9b8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case our date ranges dont match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f483544-675c-4241-b29e-579957444f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import argrelextrema\n",
    "\n",
    "# Reset the index to convert DateTimeIndex to a regular column\n",
    "vrax_df = vrax_df.reset_index()\n",
    "vrax_df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "# Calculate local maxima and minima\n",
    "vrax_df['Local_Max'] = vrax_df.iloc[argrelextrema(df['Close'].values, np.greater_equal, order=5)[0]]['Close']\n",
    "vrax_df['Local_Min'] = vrax_df.iloc[argrelextrema(df['Close'].values, np.less_equal, order=5)[0]]['Close']\n",
    "\n",
    "# Calculate the average rate of change\n",
    "vrax_df['Change'] = vrax_df['Close'].diff()\n",
    "average_rate_of_change = vrax_df['Change'].mean()\n",
    "\n",
    "# Calculate the duration of trends (number of data points between local maxima and minima)\n",
    "local_extrema_indices = sorted(vrax_df.dropna(subset=['Local_Max', 'Local_Min']).index)\n",
    "if len(local_extrema_indices) > 1:\n",
    "    durations = np.diff(local_extrema_indices)\n",
    "    average_duration = durations.mean()\n",
    "else:\n",
    "    average_duration = np.nan  # or handle appropriately if no durations are found\n",
    "\n",
    "print(\"Average Rate of Change:\", average_rate_of_change)\n",
    "print(\"Average Duration of Trends:\", average_duration)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(vrax_df.info())\n",
    "print(vrax_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b2508-65ab-43d0-b141-937f34486f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(vrax_df['Date'], vrax_df['Close'], label='Close Price', color='blue')\n",
    "\n",
    "# Plot local maxima\n",
    "plt.scatter(vrax_df['Date'], vrax_df['Local_Max'], label='Local Max', color='green', marker='^', alpha=1)\n",
    "\n",
    "# Plot local minima\n",
    "plt.scatter(vrax_df['Date'], vrax_df['Local_Min'], label='Local Min', color='red', marker='v', alpha=1)\n",
    "\n",
    "plt.title('Stock Prices with Local Maxima and Minima')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f3980-4b6e-4373-aa93-9633e3ceab88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.signal import argrelextrema, savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data: replace this with your actual stock data\n",
    "vrax_df = pd.DataFrame({\n",
    "    'Close': np.random.randn(100).cumsum()\n",
    "}, index=pd.date_range(start='1/1/2020', periods=100))\n",
    "\n",
    "# Reset the index to convert DateTimeIndex to a regular column\n",
    "vrax_df = vrax_df.reset_index()\n",
    "vrax_df.rename(columns={'index': 'Date'}, inplace=True)\n",
    "\n",
    "# Smooth the data using a moving average\n",
    "window_size = 5\n",
    "vrax_df['Close_MA'] = vrax_df['Close'].rolling(window=window_size).mean()\n",
    "\n",
    "# Smooth the data using a Savitzky-Golay filter\n",
    "window_size = 11  # window size should be odd\n",
    "poly_order = 2  # polynomial order\n",
    "vrax_df['Close_SG'] = savgol_filter(vrax_df['Close'], window_size, poly_order)\n",
    "\n",
    "# Calculate local maxima and minima on the smoothed data (using Savitzky-Golay filter as an example)\n",
    "vrax_df['Local_Max'] = vrax_df.iloc[argrelextrema(vrax_df['Close_SG'].values, np.greater_equal, order=5)[0]]['Close_SG']\n",
    "vrax_df['Local_Min'] = vrax_df.iloc[argrelextrema(vrax_df['Close_SG'].values, np.less_equal, order=5)[0]]['Close_SG']\n",
    "\n",
    "# Calculate the average rate of change\n",
    "vrax_df['Change'] = vrax_df['Close_SG'].diff()\n",
    "average_rate_of_change = vrax_df['Change'].mean()\n",
    "\n",
    "# Calculate the duration of trends (number of data points between local maxima and minima)\n",
    "local_extrema_indices = sorted(vrax_df.dropna(subset=['Local_Max', 'Local_Min']).index)\n",
    "if len(local_extrema_indices) > 1:\n",
    "    durations = np.diff(local_extrema_indices)\n",
    "    average_duration = durations.mean()\n",
    "else:\n",
    "    average_duration = np.nan  # or handle appropriately if no durations are found\n",
    "\n",
    "print(\"Average Rate of Change:\", average_rate_of_change)\n",
    "print(\"Average Duration of Trends:\", average_duration)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(vrax_df['Date'], vrax_df['Close'], label='Close Price', color='blue', alpha=0.5)\n",
    "plt.plot(vrax_df['Date'], vrax_df['Close_MA'], label='Moving Average', color='orange')\n",
    "plt.plot(vrax_df['Date'], vrax_df['Close_SG'], label='Savitzky-Golay Filter', color='green')\n",
    "\n",
    "# Plot local maxima and minima\n",
    "plt.scatter(vrax_df['Date'], vrax_df['Local_Max'], label='Local Max (SG)', color='red', marker='^')\n",
    "plt.scatter(vrax_df['Date'], vrax_df['Local_Min'], label='Local Min (SG)', color='purple', marker='v')\n",
    "\n",
    "plt.title('Stock Prices with Smoothing')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89e2fc-d7b1-4af5-baa2-f13708a1e56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Below Code Is For Testing Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d69aec5-7857-4395-a9e7-cb71ac42507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Scrape\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f3fa04-6329-4891-800a-db8732606cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = 'TGB'\n",
    "my_url = f'https://finance.yahoo.com/quote/{symbol}'\n",
    "\n",
    "\n",
    "response = requests.get(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f78a5d3f-4c29-45a7-bc88-6007d514dc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response.ok : True , response.status_code : 200\n"
     ]
    }
   ],
   "source": [
    "print(\"response.ok : {} , response.status_code : {}\".format(response.ok , response.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bfa3cee-ca52-418c-9de4-d13304e9a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of response.text :  <!doctype html>\n",
      "<html lang=\"en-US\" theme=\"light\" data-color-scheme=\"light\" class=\"desktop neo-green dock-upscale failsafe\">\n",
      "    <head>\n",
      "        <meta charset=\"utf-8\">\n",
      "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "        <meta name=\"oath:guce:consent-host\" content=\"guce.yahoo.com\">\n",
      "        \n",
      "\t\t<link href=\"../../assets/_app/immutable/assets/2.CaHVP_4h.css\" rel=\"stylesheet\">\n",
      "\t\t<link href=\"../../assets/_app/immutable/assets/Ads.3RMLh2mX.css\" rel=\"stylesheet\">\n",
      "\t\t<link hr\n"
     ]
    }
   ],
   "source": [
    "print(\"Preview of response.text : \", response.text[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9484963-f7ef-486f-b180-b4d09b4274ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url):\n",
    "    \"\"\"Download a webpage and return a beautiful soup doc\"\"\"\n",
    "    response = requests.get(url)\n",
    "    if not response.ok:\n",
    "        print('Status code:', response.status_code)\n",
    "        raise Exception('Failed to load page {}'.format(url))\n",
    "    page_content = response.text\n",
    "    doc = BeautifulSoup(page_content, 'html.parser')\n",
    "    return doc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3526936-fd52-457d-afa9-f830ee5a31b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m symbol \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTGB\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m my_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://finance.yahoo.com/quote/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mget_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m title \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m div_tags \u001b[38;5;241m=\u001b[39m doc\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mget_page\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_page\u001b[39m(url):\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download a webpage and return a beautiful soup doc\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m response\u001b[38;5;241m.\u001b[39mok:\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatus code:\u001b[39m\u001b[38;5;124m'\u001b[39m, response\u001b[38;5;241m.\u001b[39mstatus_code)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "symbol = 'TGB'\n",
    "my_url = f'https://finance.yahoo.com/quote/{symbol}'\n",
    "\n",
    "doc = get_page(my_url)\n",
    "\n",
    "title = doc.find('title')\n",
    "div_tags = doc.find_all('div', {'class': \"content\"})\n",
    "\n",
    "print(title)\n",
    "#print(div_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1f8a3b4-2923-47ea-946d-ff4e4a1769f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Reuters â€¢ 7 days ago\n",
      "Headline: Canada's Taseko shares fall 8% after strike at Gibraltar mine\n",
      "Link: https://finance.yahoo.com/news/canadas-taseko-shares-fall-8-183903487.html\n"
     ]
    }
   ],
   "source": [
    "# This code searches the div_tags to find each article and title\n",
    "print(\"Source: {}\".format(div_tags[0].find('div', {'class': \"publishing\"}).text))\n",
    "print(\"Headline:\", div_tags[0].find('h3', {'class': \"clamp\"}).text)\n",
    "\n",
    "link_tag = div_tags[0].find('a')\n",
    "if link_tag:\n",
    "    link_href = link_tag.get('href')\n",
    "    print(\"Link: {}\".format(link_href))\n",
    "else:\n",
    "    print(\"No link found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7865e804-f185-4ae7-be8e-c7b46f1ccbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc is a beautifulSoup doc item that is created in the get_page function\n",
    "def get_news_data(url):\n",
    "    # Send HTTP request to the URL\n",
    "    response = requests.get(url)\n",
    "    # Parse the HTML content of the page\n",
    "    doc = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    div_tags = doc.find_all('div', {'class': \"content\"})\n",
    "\n",
    "    # Initialize a dictionary to store the data\n",
    "    data = {}\n",
    "\n",
    "    # Extract the source\n",
    "    data['source'] = (div_tags[0].find('div', {'class': \"publishing\"}).text)\n",
    "    #source = doc.find('div', {'class': 'publishing'})\n",
    "    #data['source'] = source #.text.strip() if source else 'No source found'\n",
    "\n",
    "    # Extract the headline\n",
    "    headline = div_tags[0].find('h3', {'class': \"clamp\"}).text\n",
    "    data['headline'] = headline if headline else 'No headline found'\n",
    "\n",
    "    # Extract the link\n",
    "    link = doc.find('a', {'class': 'subtle-link'})\n",
    "    data['link'] = link['href'] if link else 'No link found'\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4636963b-0ab0-43f9-ab74-743d55700d43",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_news_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 4\u001b[0m, in \u001b[0;36mget_news_data\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_news_data\u001b[39m(url):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# Send HTTP request to the URL\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# Parse the HTML content of the page\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     doc \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "get_news_data(my_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c022bbc2-f6f2-47ef-955c-8b8cfaef8b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
