----- 6/23/2024 -----
SG
--TODAYS GOALS--
		The goal for today is to write code to test for a correlation between "the price of a commodity" and "the stock price" of a company
	whose main business is closely related to said commodity. (If P then S). To be more precise (If P inc then S inc).

	For any question realated to commodities, I typically use Taseko Mines (TGB) as my test case. I choose TGB because they own a copper mine, of the largest in the US, full stop. That is all they do. So they are the simplest case in my opinion. This will obviously be more thouroghly tested. Today is simply can we ouptut an answer and does the output make sense. 

	-PLAN-
		1 - Ingest data for TASEKO MINES and Copper Prices.
		2 - Store "End of Day" Prices for each in NUMPY ARRAYS.
		3 - Run analysis on each of these sets of data.
				a -  Using "End of Day" prices assess trend of prices and compare them with stock prices.



	NOTES FROM THE DAY
	Should not have ever assumed that I could have completed all of this today. Its Sunday. On the bright side I have learned a lot and did make some progress. I have first part of the data injestion coded up. The new funcion "av_pull", pulls data from alpha vantage and saves the data as a json file. This was done for a couple of different reason. First, i want minimize the pings to alpha vantage. As a free account there are limitations to the number api request we can ping them for each day. Secondly, offline data allows for offline analysis.

	--NEW FUNCTION--

	av_pull -- {symbol, interval, apikey}
			- {symbol} = the stock symbol for uou particular stock
					EX: Taseko Mines mines {symbol} = TGB

			- {interval} = the length of intervals we would like to request from alph_vantage. The example i am using is requesting
				all transactions to be in 1 min intervals. 
					NOTE -- in testing for taseko mines it is pulling in larger intervals. I do believe this is becuase there aren't
						as many transactions as some something like NVIDIA.
					EX: 1 Minute Interval -> '1min' = {interval}

			- {apikey} = is the key we use for accessing alpha vantage api.
					EX: 5623H2384H108384H2123 = apikey
					NOTE--As of right now this would need to requested form Steven directly.


----- 6/24/2024 -----
SG
--TODAYS GOAL--
		Todays goal is to finish creating the functions needed to completely injest data and prepare it for analysis. Yesterday's goals were
	ambitious. I need to be more thourogh in my though process. Instead of jumping into analysis i should be laying the infrastructure for all possible goals. This means creating injested tools for different types of data and preparing the database needed to perform accurate and good queries of data. This means not just storing json files, but putting those files into a database for proper data storage.

	--PLAN--
	Create the following functions:
		json_to_database
			- this function will update the database with any json data

		database_check
			- this function will check the database for any data related to the requested stock. If data exists it will display the date that
			the data was last updated. The purpose of this is to give the user the option to update the data or use it as is. 

	NOTES FROM TODAY
		I was thinking that i might need to work on my database schema and create the requisite database before writing the functions outlined in the PLAN for today.

----- 6/30/2024 -----
SG
--TODAYS GOAL--
	I am working on getting the database up and running. Since we are not interested in day trading, are columns are going to be 
	ID, Date, Open Price, Close Price, Daily Price Change. 

	I am only going to store the daily prices in the database. If I or someone wants to do a more granular analysis they can request one. 

		Note: In the final release of this there will be buttons that will allow the user to make a request from yfinance API for information that is as granular as 1min intervals. For now I will stick with daily open and close and the price change that day will be calculated as well.


ACCOMPLISHED -- Database called fin_data has now been created with the columns, symbol; date; open_price; close_price, daily_change;.

----- 07/01/2024 -----
SG
--TODAYS GOALS--
	Today's goal is to test how easy it is to implement yfinance in my data aquisition. I want to also finish the initial code for data aquisition/pre-processing. The new code will contain all the pre-processing to get the data ready for addition to the database. Using "open" price, and "close" price, i will calculate daily change. 


	NOTES FROM TODAY:
	 Yfinance has been far easier to use. NO api key is required and the data is up to date. Testing for initial download went smoothly. Will change over all code to use yfinance instead of alphavantage.

ACCOMPLISHED -- yfinance was tested and it worked really easily.


----- 07/02/2024 -----
SG
--TODAYS GOALS--
	Today's main goal is to write the code that will take the yfinance data and add it to the fin_data database. It will look for duplicate data. It will also calculate the daily change from the open an close prices.

	I will also be deleting all code related to alpha vantage.

	NOTES--
	This did not get finished today.

----- 07/04/2024 -----
SG
--TODAYS GOALS--
	Finish goals from "07/02/2024"
	Change primary key of database from ID to stock "symbol". In "stocks" table of database - "fin_data".

ACCOMPLISHED -- 
	ID column was dropped from database
	Primary key was changed and is now (symbol, date)
		-- The above two were implemented at the database level.

	dataframe that comes from yfinance is altered upon download, it removes the columns "High" and "Low". It adds the column "daily_change" that is , daily_change = "Close" - "Open".
	GOAL from --07/02/2024 is complete
		-- Data can be downloaded now and added to the 'stocks' table in 'fin_data' db. The code removes High and Low columns and adds the daily change column. It also adjust the 'date' column in the dataframe and make that column called "Date". Before it had no name. The database handles all duplicates as intended.

		Today was a good day!!
---------------


----- 07/05/2024 -----
SG
--TODAYS GOAL--
	Update database table "stocks". I want to add a column called "percent_change". This column will contain the daily percent change. It is calculated by;

										((Open / Close) - 1) * 100

										The above equation was wrong, it should be

										((Close - Open) / Open) * 100

	I want to start populating my database with the top 100 list of stocks. Then I will move on to commodities today. I do not think i will be able to get to running any analysis today.



--ACCOMPLISHED--
	I updated the database "stocks" with the new column "percent_change". 
							percent_change = ((Close - Open) / Open) * 100

	I successfuly populated "stocks" with ~ the top 100 NSYE stocks.
	I was also able to get pgadmin setup on my laptop.



----- 07/06/2024 -----
SG
--TODAYS GOAL--
	Begin downloading commodities data.
	Create new file to hold functions for data acquisition.


--NOTES--
	I need to install a new package to handle importing .ipynb files. Apparantely when you attempt to import functions from another pythong file, the import function by default is looking for .py files and not .ipynb files. So the package "import-ipynb" needs to be installed. 

	I am going to commit but i need to change the file for data_acquisition to .py

	I ended up not installing that new package. I instead moved the function to a new file called fin_func.py. No new package was required to run this. I did need to update the database and restart the kernal so that is could run correctly.

--ACCOMPLISHED--
	First commodities data was successfuly downloaded to the table 'commodities'.
	New file was created called fin_func.py -- this file contains ths functions needed to run fin_software.
	Deleted old file called 'fin_func.ipynb'.

--------

----- 07/08/2024 -----
SG
--TODAYS GOAL--
	Design Database

--NOTES--
	Didn't finish this today

----- 07/09/2024 -----
SG
--TODAYS GOAL--
	Finish database design.
	Create new tables based on database design.
	Write code for pulling in article text.
--ACCOMPLISHED--
	Initial Database design is complete. This was done in LucidChart. PDF downloaded.
	Tables created for database. Current List:
								daily_stock_prices
								daily_commod_prices
								company_list
								commodities_list
								article_list
								article_data

--NOTES--
	****I am currently trying to implement code that will scrape news articles from sites and perform semantic analysis. My initial URL took me to the general news page for yahoo finance. This is all well and good but i need to be able to search for specific companies. I found the url I want to use.

	my_url = 'https://finance.yahoo.com/quote/'place_stock_symbol_here'/news/'


	******** the above section is wrong ********* SG

	the url should be:

	my_url = 'https://finance.yahoo.com/quote/(symbol)'


	UPDATE--
	I have been able to write the correct code to find the titles for each article, where the article comes from, and the link associated with each story.

	






					